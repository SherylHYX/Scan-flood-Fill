## Scan-flood Fill README

Explanations can be seen in our paper: file 56.pdf.

### 1 Requirements

- python 2.7
- numpy
- cv2 (opencv)
- skimage
- time
- os

### 2 Dataset

​	The input images for our toy experiment are generated by dilation of eight 200 x 200  basic images.(see code dilation.py and \data\toy_example\input_200\)  Define the property 3-tuple of an image to be (multiple, border, holes), whose entries are all Boolean variables, to represent each of these eight cases. For example, (F,F,T)  means that the image has only one object inside, has no regions whose boundaries meet the border of the whole image but has "holes"  inside regions. This notation will be used to illustrate the comparison on toy examples. To show that our algorithm can also be useful in practical ground truth masks generating process, we also consider images in MSRA10K.

### 3 Overview

​	This Scan-flood Fill algorithm takes in folders of boundary images and output the following:

- mask  images. 

- running time for the model, both in terminal and in .csv files.(most python files)

​	Scan-flood Fill algorithm is an an efficient automatic precise region filling algorithm for complicated regions with the following advantages:

- In previous works, seed filling algorithms such as flood filling algorithm from OpenCV and boundary filling algorithms have been applied to generate filled masks \cite{bradski2008learning,howse2013opencv}. Although these are used in part of our approach, there exists a considerable difference in that our {\bf starting seeds are automatically provided} by the algorithm instead of being given beforehand. We propose to integrate seed filling algorithms and scan-line filling algorithms, together with the properties of adjacent connected regions, for complicated arbitrary region filling problems. 

- Besides, Scan-flood Fill(SCAFF) enables us to handle complicated regions such as a pig face and the examples from MSRA10K \cite{5}. This is by virtue of the classification of pixels, especially label pixels, and the Crop-and-"Inverse" process. We compare Scan-flood Fill(SCAFF) with its basic version EFCI for region filling. The results in Figure \ref{fig:Comparison} demonstrates the potential superiority of more precise algorithms taking the relationship between adjacent connected regions into account.

- Also, in Scan-flood Fill(SCAFF) algorithm, we do not need to worry about the potential existence of {\bf multiple regions inside an image}. This is because our filling algorithm does not require human-provided starting seeds, but can set starting seeds whenever needed. Moreover, since we start from filling exterior label color to the outermost exterior of an image, which is part of the background for the final result, we are able to avoid being trapped in a small region. 

- Moreover, padding with background color guarantees the robust filling result regardless of whether some boundaries of the regions in an image lie on the border of the whole image. 

### 4 Toy examples

​	We generate different sizes of input images by dilation of eight 200 x 200 basic images. The dilation code is Scan-flood-Fill/data/toy_examples/dilation.py, with input Scan-flood-Fill/data/toy_examples/input_200, and output is Scan-flood-Fill/data/toy_examples/output_toy.

​	We record time consumption for both EFCI and Scan-flood Fill and output corresponding .csv files. The corresponding codes are Scan-flood-Fill/filling_algorithms/EFCI_time_size.py and Scan-flood-Fill/filling_algorithms/Scan_flood_fill_time_size.py. The file Scan-flood-Fill/filling_algorithms/Scan_flood_fill_time_size_crop_first.py is the algorithm for Scan-flood-Fill, but it does cropping before the Main Filling Process. The resulting .csv files recording running time are EFCI_time_size.csv and Scan_flood_Fill_time_size.csv. The resulting masks are Scan-flood-Fill/data/toy_examples/output_EFCI_toy and Scan-flood-Fill/data/toy_examples/output_Scan_flood_Fill_toy(Scan-flood-Fill/data/toy_examples/output_Scan_flood_Fill_crop_first_toy for the cropping before Main Filling Process version).

​	EFCI code is given below:
```python
import numpy as np
import cv2
import time
import os

INPUT_PATH = 'F:/Scan-flood-Fill/data/toy_examples/input_toy/'
OUTPUT_PATH_FILL = 'F:/Scan-flood-Fill/data/toy_examples/output_EFCI_toy/'
background_threshold = 50
# pixel with difference less than this value with the background colour is seen as background colour

def del_file(path):
    # delete files
    for i in os.listdir(path):
        path_file = os.path.join(path, i)
        if os.path.isfile(path_file):
            os.remove(path_file)
        else:
            del_file(path_file)

def pad_with(vector, pad_width, iaxis, kwargs):
    pad_value = kwargs.get('padder', 0)
    vector[:pad_width[0]] = pad_value
    vector[-pad_width[1]:] = pad_value
    return vector

def readAndPad(imagePath, backGroundColour):
    img = cv2.imread(imagePath, 0)
    padImg = np.pad(img,1,pad_with, padder=backGroundColour)
    height, width = padImg.shape[:2]
    return padImg, height, width

def floodFill(img, height, width, x, y, boundaryColour, backGroundColour, fillColour):
    img[x, y] = fillColour
    if (x>0 and img[x-1, y] == backGroundColour):
        floodFill(img, height, width, x-1, y, boundaryColour, backGroundColour, fillColour)
    if (y>0 and img[x, y-1] == backGroundColour):
        floodFill(img, height, width, x, y-1, boundaryColour, backGroundColour, fillColour)
    if (x<(height-1) and img[x+1, y] == backGroundColour):
        floodFill(img, height, width, x+1, y, boundaryColour, backGroundColour, fillColour)
    if (y<(width-1) and img[x, y+1] == backGroundColour):
        floodFill(img, height, width, x, y+1, boundaryColour, backGroundColour, fillColour)

def cvFloodFill(img, height, width, fillColour):
    mask = np.zeros([height+2, width +2], np.uint8)
    cv2.floodFill(img, mask, (0, 0), newVal = fillColour, loDiff = 50, upDiff = 50, flags = 4)

def cropAndReverse(img, height, width, maskColour, backGroundColour, fillColour):
    croppedImg = np.delete((np.delete(img, [0, width-1], axis=1)), [0, height-1], axis=0)
    for x in range(height-2):
        for y in range(width-2):
            # fillColour is the color that is temporarily used to fill exterior
            # maskColour is for the final result (interior)
            if croppedImg[x,y] == fillColour:
                croppedImg[x,y] = backGroundColour
            elif abs(croppedImg[x,y] - backGroundColour) < background_threshold:
                croppedImg[x,y] = maskColour
    return croppedImg

def fillBoundaryMain(imagePath ,savePath):

    padImg, height, width = readAndPad(imagePath, backGroundColour = 0)
    cvFloodFill(padImg, height, width, 128)
    filledImg = cropAndReverse(padImg, height, width, 255, 0, 128)
    cv2.imwrite(savePath, filledImg, [int(cv2.IMWRITE_JPEG_QUALITY),100])



def fillBoundary_main():
    fill_time = np.zeros([10])
    for i, n in enumerate(range(200, 2200, 200)):
        output_folder_name = 'original_size_' + str(n)
        input_folder_name = 'input_' + str(n)
        input_path = os.path.join(INPUT_PATH, input_folder_name)
        output_path_fill = os.path.join(OUTPUT_PATH_FILL, output_folder_name)
        # delete previous files
        os.mkdir(output_path_fill)
        #uncomment to generate folder for the results
        del_file(output_path_fill)
        start = time.clock()
        for root, dirs, files in os.walk(input_path):
            for f in files:
                fillBoundaryMain(os.path.join(input_path, f), os.path.join(output_path_fill, f))
        elapsed = (time.clock() - start)
        print("Time used for size {} is:".format(str(n)))
        print(elapsed)
        fill_time[i] = elapsed
    print(fill_time)
    np.savetxt("EFCI_time_size.csv", fill_time, delimiter=",")
    
    if __name__ == '__main__':
    fillBoundary_main()
```

​	Scan-flood Fill code is given below:
```python
import numpy as np
import cv2
import time
import os

INPUT_PATH = 'F:/Scan-flood-Fill/data/toy_examples/input_toy/'
OUTPUT_PATH_FILL = 'F:/Scan-flood-Fill/data/toy_examples/output_Scan_flood_Fill_toy/'
colorThreshold = 35

def del_file(path):
    # delete files
    for i in os.listdir(path):
        path_file = os.path.join(path, i)
        if os.path.isfile(path_file):
            os.remove(path_file)
        else:
            del_file(path_file)

def pad_with(vector, pad_width, iaxis, kwargs):
    pad_value = kwargs.get('padder', 0)
    vector[:pad_width[0]] = pad_value
    vector[-pad_width[1]:] = pad_value
    return vector

def readAndPad(imagePath, backGroundColour):
    img = cv2.imread(imagePath, 0)
    padImg = np.pad(img,1,pad_with, padder=backGroundColour)
    height, width = padImg.shape[:2]
    return padImg, height, width

def cropAndReverse(img, height, width, backGroundColor, labelColor, fillColor, maskColor):
    croppedImg = np.delete((np.delete(img, [0, width-1], axis=1)), [0, height-1], axis=0)
    for x in range(height-2):
        for y in range(width-2):
            # label is the color that is temporarily used to fill exterior
            if croppedImg[x,y] == labelColor:
                croppedImg[x,y] = backGroundColor
            if croppedImg[x, y] == fillColor:
                croppedImg[x, y] = maskColor
    return croppedImg

def cvFloodFill(img, height, width, seedPosition, fillColor):
    mask = np.zeros([height+2, width +2], np.uint8)
    cv2.floodFill(img, mask, seedPosition, newVal = fillColor, loDiff = 50, upDiff = 50, flags = 4)

def holesInHoles(imgPath, savePath, backGroundColor, boundaryColor, labelColor, fillColor, maskColor):
    padImg, height, width = readAndPad(imgPath, backGroundColor)
    seedPosition = (0, 0)
    cvFloodFill(padImg, height, width, seedPosition, labelColor)
    for x in range(height):
        for y in range(width):
            pixelValue = padImg[x, y]
            if abs(pixelValue - backGroundColor) < colorThreshold:
                seedPosition = (y,x)
                i = 1
                while abs(padImg[x, y-i] - boundaryColor) < colorThreshold:
                    i = i + 1
                if abs(padImg[x, y-i] - labelColor) < colorThreshold:
                    cvFloodFill(padImg, height, width, seedPosition, fillColor)
                else:
                    cvFloodFill(padImg, height, width, seedPosition, labelColor)

    resultImg = cropAndReverse(padImg, height, width, backGroundColor, labelColor, fillColor, maskColor)
    cv2.imwrite(savePath, resultImg, [int(cv2.IMWRITE_JPEG_QUALITY), 100])
def holeFill_main():
    fill_time = np.zeros([10])
    boundaryColor = 255
    fillColor = 128
    labelColor = 80
    backGroundColor = 0
    maskColor = 255
    for i, n in enumerate(range(200, 400, 200)):
        output_folder_name = 'improved_size_' + str(n)
        input_folder_name = 'input_' + str(n)
        input_path = os.path.join(INPUT_PATH, input_folder_name)
        output_path_fill = os.path.join(OUTPUT_PATH_FILL, output_folder_name)
        #os.mkdir(output_path_fill)
        #uncomment to generate folder for the results
        # delete previous files
        del_file(output_path_fill)
        start = time.clock()
        for root, dirs, files in os.walk(input_path):
            for f in files:
                holesInHoles(os.path.join(input_path, f), os.path.join(output_path_fill, f), backGroundColor, boundaryColor, labelColor, fillColor, maskColor)
        elapsed = (time.clock() - start)
        print("Scan-flood Fill time used for size {} is:".format(str(n)))
        print(elapsed)
        fill_time[i] = elapsed
    print(fill_time)
    np.savetxt("Scan_flood_Fill_time_size.csv", fill_time, delimiter=",")
    
    if __name__ == '__main__':
    holeFill_main()
```

### 5 Flow chart generation

​	To illustrate the process clearly, we generate some flow charts. For EFCI, we use input Scan-flood-Fill/flow_chart/multi_input and code Scan-flood-Fill/flow_chart/EFCI_flowchart.py to obtain output Scan-flood-Fill/flow_chart/multi_output. Similarly, for Scan-flood Fill, we use input Scan-flood-Fill/flow_chart/pig_input and code Scan-flood-Fill/data/toy_examples/Scan_flood_fill_flowchart.py to obtain output Scan-flood-Fill/flow_chart/pig_output.

​	First we save corresponding numpy array for HU(the Hounsfield unit scale, see https://en.wikipedia.org/wiki/Hounsfield_scale) values into csv files. We then convert dicom files into jpg files, by normalizing HU values(512*512 pixel values in dicom files) to [0,255]. 

​	After that, we generate edgeMaps+centroids png files named after uid and mask png files by filling edgeMaps png files by straight lines. Straight lines are reasonable in that lung nodules are normally almost convex, and with centroids considered, we are less likely to fill outside the nodule area. Filling process is accompanied by morphology close to fill small holes. 

​	Via these process, we obtain four corresponding files for valid dicom files and their valid nodules (i.e.nodules that are at least 3mm in the NOT "FULL" version) as well as the "FULL" version (including small nodules). We obtain one-channel and three-channel data, and both "FULL" and NOT "FULL" versions at the same time.

​	Corresponding codes are as follows: (we use thresholding here, and this code generates "0115" and "0114" datasets)

```python
def output_pre0115_both(current_Path, validNodes, validNodes_full, patientNo):
    # judge whether this is a valid folder of CT images and proper thickness
    for f_dcm in findAllFiles(current_Path, '.dcm'):
        if not isValidDcm(f_dcm):
            return validNodes, validNodes_full
        else:
            break
    # use ET.parse() to parse xml
    for xmlPath in findAllFiles(current_Path, '.xml'):
        tree = ET.parse(xmlPath)
        root = tree.getroot()
        # To store the UID, edgeMaps, centroid of valid nodules
        # first include small nodules and save files to "FULL" folders
        for readSession in root.findall('nih:readingSession', NS):
            noduleNode = readSession.findall('nih:unblindedReadNodule', NS)
            for node in noduleNode:
                is_small_nodule = 0  # Reset the judgement of whether it is a small nodule
                charNode = node.findall('nih:characteristics', NS)
                # Judge whether it is a small nodule
                if charNode is None or len(charNode) == 0:
                    is_small_nodule = 1
                    if INCLUDE_SMALL_NODULE == 0:  # modified by Tianyi
                        # Exclude nodules that have diameter less than 3mm
                        continue
                xmlRois = node.findall('nih:roi', NS)
                for roi in xmlRois:
                    # Exclude nodules that are not real
                    if roi.find('nih:inclusion', NS) is None:
                        continue
                    if roi.find('nih:inclusion', NS).text != 'TRUE':
                        continue
                    # There is abnormal instances that isn't small nodule but has less than 3 edgemaps
                    if len(roi.findall('nih:edgeMap', NS)) < 2:
                        if is_small_nodule == 0:
                            # Exclude nodules that have less than 3 (x,y).
                            continue
                    # Finally get the valid nodes
                    # Parse the UID of the nodule
                    uid = roi.find('nih:imageSOP_UID', NS).text
                    # Create the JPG of the Lung CT
                    for f_dcm in findAllFiles(current_Path, '.dcm'):
                        dcm = pydicom.dcmread(f_dcm)
                        # find the .dcm w.r.t UID in .xml
                        if dcm[0x0008, 0x0018].value == uid:
                            jpgPath = JPG_SAVE_PATH_FULL + str(patientNo) + uid.replace(".", "_") + '.jpg'
                            jpgPath_single = JPG_SAVE_PATH_FULL_single + str(patientNo) + uid.replace(".", "_") + '.jpg'
                            # modified by Yixuan
                            dcm_numpy = dcm.pixel_array
                            convert_from_dicom_to_jpg(dcm_numpy, jpgPath_single)
                            convert_from_dicom_to_jpg_3channel0114(dcm_numpy, jpgPath)
                            break  # end the loop if we have already found the corresponding dicom picture
                    # Parse the coordinates of nodule's edge map
                    edgePath = EDGE_SAVE_PATH_FULL + str(patientNo) + uid.replace(".", "_") + '.png'
                    maskPath = MASK_SAVE_PATH_FULL + str(patientNo) + uid.replace(".", "_") + '.png'
                    if not os.path.exists(edgePath):
                        # initialize .png file for edge png
                        edgePng = np.zeros((512, 512))
                    else:
                        edgePng = mpimg.imread(edgePath)
                    if not os.path.exists(maskPath):
                        # initialize .png file for mask png
                        maskPng = np.zeros((512, 512))
                    else:
                        maskPng = mpimg.imread(maskPath)

                    edgeXY = []
                    edgeMaps = roi.findall('nih:edgeMap', NS)
                    if (is_small_nodule):  # modified by Tianyi
                        # if it is a small nodule
                        for edgeMap in edgeMaps:
                            # Caution: The coordinate in EdgeMap is exactly inversed in the PNG image
                            # Modified by Tianyi 2019-01-15
                            y = int(edgeMap.find('nih:xCoord', NS).text)
                            x = int(edgeMap.find('nih:yCoord', NS).text)
                            centroid = [x, y]
                            radius = 2
                            # define the coordinate of the edge
                            edge_X = [0, 0, 1, 1, -1, -1, -2, -2, -2, 2, 2, 2]
                            edge_Y = [2, -2, 2, -2, 2, -2, 1, 0, -1, 1, 0, -1]
                            for i in range(len(edge_X)):
                                edgePng[x + edge_X[i - 1], y + edge_Y[i - 1]] = 127
                                maskPng[x + edge_X[i - 1], y + edge_Y[i - 1]] = 127
                    else:
                        for edgeMap in edgeMaps:
                            # Caution: The coordinate in EdgeMap is exactly inversed in the PNG image
                            y = int(edgeMap.find('nih:xCoord', NS).text)
                            x = int(edgeMap.find('nih:yCoord', NS).text)
                            edgePng[x][y] = 255
                            maskPng[x][y] = 255
                            edgeXY.append([x, y])
                        # Calculate the centroid
                        xMax = max([_[0] for _ in edgeXY])
                        xMin = min([_[0] for _ in edgeXY])
                        yMax = max([_[1] for _ in edgeXY])
                        yMin = min([_[1] for _ in edgeXY])
                        centroid = [(xMax + xMin) / 2, (yMax + yMin) / 2]
                    if is_small_nodule:
                        edgePng[centroid[0]][centroid[1]] = 127
                        maskPng[centroid[0]][centroid[1]] = 127
                    else:
                        edgePng[centroid[0]][centroid[1]] = 255
                        maskPng[centroid[0]][centroid[1]] = 255
                    # fill the mask, start from the centroid, along a straight line until the edge point
                    for edgePt in edgeXY:
                        edgeX = edgePt[0]
                        edgeY = edgePt[1]
                        stepNo = max(abs(edgeX - centroid[0]), abs(edgeY - centroid[1]), 10)
                        # number of steps, abs is small, we need another integer
                        delX = float(edgeX - centroid[0]) / stepNo
                        delY = float(edgeY - centroid[1]) / stepNo
                        for i in range(stepNo):
                            if (is_small_nodule):
                                maskPng[int(centroid[0] + i * delX)][int(centroid[1] + i * delY)] = 127
                            else:
                                maskPng[int(centroid[0] + i * delX)][int(centroid[1] + i * delY)] = 255
                    # Store the UID, edgeMaps, centroid of valid nodules
                    if uid in validNodes_full.keys():
                        validNodes_full[uid].append([edgeXY, centroid])
                    else:
                        validNodes_full[uid] = [edgeXY, centroid]
                    # edgePng only contains the edgeMaps, while maskPng contains the full mask
                    cv2.imwrite(edgePath, edgePng.astype('uint8'), [int(cv2.IMWRITE_PNG_COMPRESSION), 0])
                    kernel = np.ones((5, 5), np.uint8)
                    maskPng = cv2.morphologyEx(maskPng.astype('uint8'), cv2.MORPH_CLOSE, kernel)
                    # use morphology close to fill the small holes
                    cv2.imwrite(maskPath, maskPng.astype('uint8'), [int(cv2.IMWRITE_PNG_COMPRESSION), 0])

        # then exclude small nodules and save files to those "without 'FULL'" folders
        for readSession in root.findall('nih:readingSession', NS):
            noduleNode = readSession.findall('nih:unblindedReadNodule', NS)
            for node in noduleNode:
                # Exclude nodules that have diameter less than 3mm
                charNode = node.findall('nih:characteristics', NS)
                if charNode is None or len(charNode) == 0:
                    continue
                xmlRois = node.findall('nih:roi', NS)
                for roi in xmlRois:
                    # Exclude nodules that are not real
                    if roi.find('nih:inclusion', NS) is None:
                        continue
                    if roi.find('nih:inclusion', NS).text != 'TRUE':
                        continue
                    # Exclude nodules that have less than 3 (x,y).
                    if len(roi.findall('nih:edgeMap', NS)) < 2:
                        continue
                    # Finally get the valid nodes
                    # Parse the UID of the nodule
                    uid = roi.find('nih:imageSOP_UID', NS).text
                    # Create the JPG of the Lung CT
                    for f_dcm in findAllFiles(current_Path, '.dcm'):
                        dcm = pydicom.dcmread(f_dcm)
                        if dcm[0x0008, 0x0018].value == uid:
                            jpgPath = JPG_SAVE_PATH + str(patientNo) + uid.replace(".", "_") + '.jpg'
                            jpgPath_single = JPG_SAVE_PATH_single + str(patientNo) + uid.replace(".", "_") + '.jpg'
                            # modified by Yixuan
                            dcm_numpy = dcm.pixel_array
                            convert_from_dicom_to_jpg(dcm_numpy, jpgPath_single)
                            convert_from_dicom_to_jpg_3channel0114(dcm_numpy, jpgPath)
                            break  # end the loop if we have already found the corresponding dicom picture
                    # Parse the coordinates of nodule's edge map
                    edgePath = EDGE_SAVE_PATH + str(patientNo) + uid.replace(".", "_") + '.png'
                    maskPath = MASK_SAVE_PATH + str(patientNo) + uid.replace(".", "_") + '.png'
                    if not os.path.exists(edgePath):
                        edgePng = np.zeros((512, 512))
                    else:
                        edgePng = mpimg.imread(edgePath)
                    if not os.path.exists(maskPath):
                        maskPng = np.zeros((512, 512))
                    else:
                        maskPng = mpimg.imread(maskPath)
                    edgeXY = []
                    edgeMaps = roi.findall('nih:edgeMap', NS)

                    for edgeMap in edgeMaps:
                        # Caution: The coordinates in EdgeMap are exactly inversed in the PNG image
                        y = int(edgeMap.find('nih:xCoord', NS).text)
                        x = int(edgeMap.find('nih:yCoord', NS).text)
                        edgePng[x][y] = 255
                        maskPng[x][y] = 255
                        edgeXY.append([x, y])
                    # Calculate the centroid
                    xMax = max([_[0] for _ in edgeXY])
                    xMin = min([_[0] for _ in edgeXY])
                    yMax = max([_[1] for _ in edgeXY])
                    yMin = min([_[1] for _ in edgeXY])
                    centroid = [(xMax + xMin) / 2, (yMax + yMin) / 2]
                    edgePng[centroid[0]][centroid[1]] = 255
                    maskPng[centroid[0]][centroid[1]] = 255
                    # fill the mask, start from the centroid, along a straight line until the edge point
                    for edgePt in edgeXY:
                        edgeX = edgePt[0]
                        edgeY = edgePt[1]
                        stepNo = max(abs(edgeX - centroid[0]), abs(edgeY - centroid[1]), 10)
                        # number of steps, abs is small, we need another integer
                        delX = float(edgeX - centroid[0]) / stepNo
                        delY = float(edgeY - centroid[1]) / stepNo
                        for i in range(stepNo):
                            maskPng[int(centroid[0] + i * delX)][int(centroid[1] + i * delY)] = 255
                     # Store the UID, edgeMaps, centroid of valid nodules
                    if uid in validNodes.keys():
                        validNodes[uid].append([edgeXY, centroid])
                    else:
                        validNodes[uid] = [edgeXY, centroid]
                     #edgePng only contains the edgeMaps, while maskPng contains the FULL mask
                    cv2.imwrite(edgePath, edgePng.astype('uint8'), [int(cv2.IMWRITE_PNG_COMPRESSION), 0])
                    kernel = np.ones((5, 5), np.uint8)
                    maskPng = cv2.morphologyEx(maskPng.astype('uint8'), cv2.MORPH_CLOSE, kernel)
                    # use morphology close to fill the small holes
                    cv2.imwrite(maskPath, maskPng.astype('uint8'), [int(cv2.IMWRITE_PNG_COMPRESSION), 0])

    return validNodes, validNodes_fulldef output_pre(currernt_Path, validNodes):
    # use ET.parse() to parse xml
    for xmlPath in findAllFiles(currernt_Path, '.xml'):
        tree = ET.parse(xmlPath)
        root = tree.getroot()
        # To store the UID, edgeMaps, centroid of valid nodules
        for readSession in root.findall('nih:readingSession', NS):
            noduleNode = readSession.findall('nih:unblindedReadNodule', NS)
            for node in noduleNode:
                # Exclude nodules that have diameter less than 3mm
                charNode = node.findall('nih:characteristics', NS)
                if (charNode is None or len(charNode) == 0):
                    continue
                xmlRois = node.findall('nih:roi', NS)
                for roi in xmlRois:
                    # Exclude nodules that are not real
                    if roi.find('nih:inclusion', NS) is None:
                        continue
                    if roi.find('nih:inclusion', NS).text != 'TRUE':
                        continue
                    # Exclude nodules that have less than 3 (x,y).
                    if len(roi.findall('nih:edgeMap', NS)) < 2:
                        continue
                    # Finally get the valid nodes
                    # Parse the UID of the nodule
                    uid = roi.find('nih:imageSOP_UID', NS).text
                    for f_dcm in findAllFiles(currernt_Path, '.dcm'):
                        dcm = pydicom.dcmread(f_dcm)
                        if dcm[0x0008, 0x0018].value == uid:
                            jpgPath = JPG_SAVE_PATH + uid + '.jpg'
                            # modified by Yixuan
                            dcm_numpy = dcm.pixel_array
                            # save dicom numpy to csv
                            # first create the file
                            np.savetxt(NUMPY_DICOM_SAVE_PATH + uid + ".csv", dcm_numpy, delimiter=",")
                            with open(NUMPY_DICOM_SAVE_PATH + uid + ".csv", 'wb') as f:
                                writer = csv.writer(f)
                                writer.writerows(dcm_numpy)
                            convert_from_dicom_to_jpg(dcm_numpy, jpgPath)
                            break  # end the loop if we have already found the corresponding dicom picture
                    # Parse the coordinates of nodule's edge map
                    edgePath = EDGE_SAVE_PATH + uid + '.png'
                    maskPath = MASK_SAVE_PATH + uid + '.png'
                    if not os.path.exists(edgePath):
                        edgePng = np.zeros((512, 512))
                    else:
                        edgePng = mpimg.imread(edgePath)
                    if not os.path.exists(maskPath):
                        maskPng = np.zeros((512, 512))
                    else:
                        maskPng = mpimg.imread(maskPath)
                    edgeXY = []
                    edgeMaps = roi.findall('nih:edgeMap', NS)

                    for edgeMap in edgeMaps:
                        x = int(edgeMap.find('nih:xCoord', NS).text)
                        y = int(edgeMap.find('nih:yCoord', NS).text)
                        edgePng[x][y] = 255
                        maskPng[x][y] = 255
                        edgeXY.append([x, y])
                    # Calculate the centroid
                    xMax = max([_[0] for _ in edgeXY])
                    xMin = min([_[0] for _ in edgeXY])
                    yMax = max([_[1] for _ in edgeXY])
                    yMin = min([_[1] for _ in edgeXY])
                    centroid = [(xMax + xMin) / 2, (yMax + yMin) / 2]
                    edgePng[centroid[0]][centroid[1]] = 255
                    maskPng[centroid[0]][centroid[1]] = 255
                    # fill the mask, start from the centroid, along a straight line until the edge point
                    for edgePt in edgeXY:
                        edgeX = edgePt[0]
                        edgeY = edgePt[1]
                        stepNo = max(abs(edgeX - centroid[0]), abs(edgeY - centroid[1]), 10)
                        # number of steps, abs is small, we need another integer
                        delX = float(edgeX - centroid[0]) / stepNo
                        delY = float(edgeY - centroid[1]) / stepNo
                        for i in range(stepNo):
                            maskPng[int(centroid[0] + i * delX)][int(centroid[1] + i * delY)] = 255
                     # Store the UID, edgeMaps, centroid of valid nodules
                    if uid in validNodes.keys():
                        validNodes[uid].append([edgeXY, centroid])
                    else:
                        validNodes[uid] = [edgeXY, centroid]
                     #edgePng only contains the edgeMaps, while maskPng contains the full mask
                    cv2.imwrite(edgePath, edgePng.astype('uint8'), [int(cv2.IMWRITE_PNG_COMPRESSION), 0])
                    kernel = np.ones((5, 5), np.uint8)
                    maskPng = cv2.morphologyEx(maskPng.astype('uint8'), cv2.MORPH_CLOSE, kernel)
                    # use morphology close to fill the small holes
                    cv2.imwrite(maskPath, maskPng.astype('uint8'), [int(cv2.IMWRITE_PNG_COMPRESSION), 0])
    return validNodes
```

​	The converting function from dicom to one-channel jpg is:

```python
def convert_from_dicom_to_jpg(img, save_path):
    img[img < -1000] = -1000
    # From HU in [-1000,500],taking <-1000 to -1000 and >500 to 500 before normalization
    img[img > 500] = 500
    # We need float first then convert to int, otherwise we can only get black and white!
    newimg = (((img + 1000.0) / 1500.0) * 255.0).astype('uint8')
    cv2.imwrite(save_path, newimg, [int(cv2.IMWRITE_JPEG_QUALITY), 100])
```

​	For the revised version excluding small nodules, i.e. generating "LIDC_MASK_0129" and "LIDC_EDGE_0129", we have the following code:

```python
def output_pre0129(current_Path, validNodes, validNodes_full, patientNo):
    # judge whether this is a valid folder of CT images and proper thickness
    for f_dcm in findAllFiles(current_Path, '.dcm'):
        if not isValidDcm(f_dcm):
            return validNodes, validNodes_full
        else:
            break
    # use ET.parse() to parse xml
    for xmlPath in findAllFiles(current_Path, '.xml'):
        tree = ET.parse(xmlPath)
        root = tree.getroot()
        # exclude small nodules and save files to those "without 'FULL'" folders
        for readSession in root.findall('nih:readingSession', NS):
            noduleNode = readSession.findall('nih:unblindedReadNodule', NS)
            for node in noduleNode:
                # Exclude nodules that have diameter less than 3mm
                charNode = node.findall('nih:characteristics', NS)
                if charNode is None or len(charNode) == 0:
                    continue
                xmlRois = node.findall('nih:roi', NS)
                for roi in xmlRois:
                    # Exclude nodules that are not real
                    if roi.find('nih:inclusion', NS) is None:
                        continue
                    if roi.find('nih:inclusion', NS).text != 'TRUE':
                        continue
                    # Exclude nodules that have less than 3 (x,y).
                    if len(roi.findall('nih:edgeMap', NS)) < 2:
                        continue
                    # Finally get the valid nodes
                    # Parse the UID of the nodule
                    uid = roi.find('nih:imageSOP_UID', NS).text
                    # Parse the coordinates of nodule's edge map
                    edgePath = EDGE_SAVE_PATH + str(patientNo) + uid.replace(".", "_") + '.png'
                    maskPath = MASK_SAVE_PATH + str(patientNo) + uid.replace(".", "_") + '.png'
                    if not os.path.exists(edgePath):
                        edgePng = np.zeros((512, 512))
                    else:
                        edgePng = mpimg.imread(edgePath)
                    if not os.path.exists(maskPath):
                        maskPng = np.zeros((512, 512))
                    else:
                        maskPng = mpimg.imread(maskPath)
                    edgeXY = []
                    edgeMaps = roi.findall('nih:edgeMap', NS)

                    for edgeMap in edgeMaps:
                        # Caution: The coordinates in EdgeMap are exactly inversed in the PNG image
                        y = int(edgeMap.find('nih:xCoord', NS).text)
                        x = int(edgeMap.find('nih:yCoord', NS).text)
                        edgePng[x][y] = 255
                        maskPng[x][y] = 255
                        edgeXY.append([x, y])
                    # Calculate the centroid
                    xMax = max([_[0] for _ in edgeXY])
                    xMin = min([_[0] for _ in edgeXY])
                    yMax = max([_[1] for _ in edgeXY])
                    yMin = min([_[1] for _ in edgeXY])
                    centroid = [(xMax + xMin) / 2, (yMax + yMin) / 2]
                    edgePng[centroid[0]][centroid[1]] = 255
                    maskPng[centroid[0]][centroid[1]] = 255
                    # fill the mask, start from the centroid, along a straight line until the edge point
                    for edgePt in edgeXY:
                        edgeX = edgePt[0]
                        edgeY = edgePt[1]
                        stepNo = max(abs(edgeX - centroid[0]), abs(edgeY - centroid[1]), 10)
                        # number of steps, abs is small, we need another integer
                        delX = float(edgeX - centroid[0]) / stepNo
                        delY = float(edgeY - centroid[1]) / stepNo
                        for i in range(stepNo):
                            maskPng[int(centroid[0] + i * delX)][int(centroid[1] + i * delY)] = 255
                     # Store the UID, edgeMaps, centroid of valid nodules
                    if uid in validNodes.keys():
                        validNodes[uid].append([edgeXY, centroid])
                    else:
                        validNodes[uid] = [edgeXY, centroid]
                     #edgePng only contains the edgeMaps, while maskPng contains the FULL mask
                    cv2.imwrite(edgePath, edgePng.astype('uint8'), [int(cv2.IMWRITE_PNG_COMPRESSION), 0])
                    kernel = np.ones((5, 5), np.uint8)
                    maskPng = cv2.morphologyEx(maskPng.astype('uint8'), cv2.MORPH_CLOSE, kernel)
                    # use morphology close to fill the small holes
                    cv2.imwrite(maskPath, maskPng.astype('uint8'), [int(cv2.IMWRITE_PNG_COMPRESSION), 0])

    return validNodes, validNodes_full
```

​	For the three-channel case, we have: (we use color red to stand for low HU values, green for middle, and blue for high, within thresholding ranges)

```python 
def convert_from_dicom_to_jpg_3channel0114(img, save_path):
    img[img < -1024] = -1024
    #  for HU not in [-1024,511], we take HU <-1024 to -1024 and HU >511 to 511 before normalization
    img[img > 511] = 511
    # We need float first then convert to int, otherwise we can only get black and white!
    multi_channel_img = np.zeros((img.shape[0], img.shape[1], 3))
    # Colours in JPG are strored in this order [blue, green, red]
    # Initialize to [0,0,0]
    for x in range(512):
        for y in range(512):
            pixelvalue = img[x,y]
            # red channel
            if  pixelvalue< -512:
                multi_channel_img [x,y,2] = (( pixelvalue + 1024.0)/2.0).astype(int)
            else:
                if  pixelvalue < 0:
                    # green channel
                    multi_channel_img [x,y,1] = (( pixelvalue + 512.0)/2.0).astype(int)
                else:
                    # blue channel
                    multi_channel_img [x,y,0] = (( pixelvalue+ 0)/2.0).astype(int)

    cv2.imwrite(save_path, multi_channel_img, [int(cv2.IMWRITE_JPEG_QUALITY), 100])
```

### 6 LIDC_MEnet

​	Our model is autonomous since we do not need to split the data, do the training and testing, renaming

and creating files by ourselves. All we need to do is to input some training proportion and then execute the file LIDC_menet_main_yx.py, which in fact consists of the following processes:

#### 6.1 Split data:

​	Data split includes clearing the origin training and testing folders, and moving new files inside.Explicitly, the training folders are: /train_data/GT_train/ for files in LIDC_MASK_FULL in the LIDC_RENAME folder, /train_data/images_train/ for files in LIDC_JPG_FULL in the LIDC_RENAME folder, /train_data/Images_train_SOBEL/ for files in LIDC_EDGE_FULL in the LIDC_RENAME folder; the testing folders are: /test_data/Images_test/ for files in LIDC_JPG_FULL in the LIDC_RENAME folder, /test_data/LIDC_EDGE/ for files in LIDC_EDGE_FULL in the LIDC_RENAME folder, /test_data/LIDC_MASK/ for files in LIDC_MASK_FULL in the LIDC_RENAME folder.

​	During this process, we make use of file names, taking different patients to either training set or test set. 

​	This process uses the following:

```python
def del_file(path):
    # delete files
    for i in os.listdir(path):
        path_file = os.path.join(path, i)
        if os.path.isfile(path_file):
            os.remove(path_file)
        else:
            del_file(path_file)

def split_data_details1(originPath, trainPath, testPath, split_proportion):
    del_file(trainPath) # first clean the directories
    del_file(testPath)
    for root, dirs, filess in os.walk(originPath):
        for files in filess:
            if(extractPatientNo(files) <= int(split_proportion * 1012)):
                os.system('cp {} {}'.format(os.path.join(originPath, files),\
                os.path.join(trainPath, files)))
            else:
                os.system('cp {} {}'.format(os.path.join(originPath, files),\
                os.path.join(testPath, files)))

def split_data1(split_proportion):
    MASK_PATH = '/workspace/LIDC_MASK0129/'
    EDGE_PATH = '/workspace/LIDC_EDGE0129/'
    JPG_PATH = '/workspace/LIDC_JPG0114_single/'

    MASK_TRAIN_PATH = '/workspace/LIDC_menet_test/LIDC_menet_model/train_data/GT_train/'
    EDGE_TRAIN_PATH = '/workspace/LIDC_menet_test/LIDC_menet_model/train_data/Images_train_SOBEL/'
    JPG_TRAIN_PATH = '/workspace/LIDC_menet_test/LIDC_menet_model/train_data/Images_train/'

    MASK_TEST_PATH = '/workspace/LIDC_menet_test/LIDC_menet_model/test_data/LIDC_MASK/'
    EDGE_TEST_PATH = '/workspace/LIDC_menet_test/LIDC_menet_model/test_data/LIDC_EDGE/'
    JPG_TEST_PATH = '/workspace/LIDC_menet_test/LIDC_menet_model/test_data/Images_test/'
    split_data_details1(MASK_PATH, MASK_TRAIN_PATH, MASK_TEST_PATH, split_proportion)
    split_data_details1(EDGE_PATH, EDGE_TRAIN_PATH, EDGE_TEST_PATH, split_proportion)
    split_data_details1(JPG_PATH, JPG_TRAIN_PATH, JPG_TEST_PATH, split_proportion)
```

#### 6.2 Training

​	We use MEnet here, where generated models can be found in the "snapthot" folder.

#### 6.3 Testing

​	we use MEnet again, where testing results can be found in the "attention_iter_110000" folder.

#### 6.4 Renaming files

​	Rename the trained model folder and testing result folder to include the training proportion and process date. Then we get:

- several caffe models in "snapshot" folder, classified by last characters denoting training proportion and process date. For example, folder "snapshot75-2019-01-03" denotes the trained model for a training proportion of 75% on the date 2019-01-03.
- several caffe models in "attention_iter_110000" folder, classified by last characters denoting training proportion and process date. For example, folder "attention_iter_11000075-2019-01-03" denotes the trained model for a training proportion of 75% on the date 2019-01-03.

#### 6.5 Create folders

​	We create the "snapshot" folder and the "attention_iter_110000" folder for the next model.

​	Therefore, the whole process can be integrated into a python file: LIDC_menet_main_1.py, which has the main function as:

```python
def main_1(sliptProportion):
    # 1. split the data
    # 1.1 exclude small nodules
    modelName = 'attention_iter_110000_' + str(int(100 * sliptProportion)) + '-' + time.strftime("%Y-%m-%d",time.localtime())
    testName = str(int(100 * sliptProportion)) + '-' + time.strftime("%Y-%m-%d", time.localtime())
    try:
        print('start split data (exclude small nodules)')
        split_data1(sliptProportion)
    except:
        print('split failed')
        return 0
    else:
        print('split done')
    # 2. training
    print('start training model: ' + modelName)
    try:
        os.system("python training.py")
    except:
        print('training fail')
        return 0
    else:
        print('training finished')
    # 3. testing
    print('start testing')
    try:
        print(testing1.GV.test_images_num)
        os.system("python testing1.py")
    except:
        print('testing fail')
        return 0
    else:
        print('testing finished')
    # 4. rename
    os.rename('/workspace/LIDC_menet_test/LIDC_menet_model/attention_iter_110000/', '/workspace/LIDC_menet_test/LIDC_menet_model/attention_iter_110000' + testName + '/')
    os.rename('/workspace/LIDC_menet_test/LIDC_menet_model/model/snapshot/', '/workspace/LIDC_menet_test/LIDC_menet_model/model/snapshot' + testName + '/')
    # 5. create new directories
    os.mkdir("/workspace/LIDC_menet_test/LIDC_menet_model/attention_iter_110000/")
    os.mkdir("/workspace/LIDC_menet_test/LIDC_menet_model/model/snapshot/")
```

In this way, we can train and test several models simply by running one python program, say, using the following:

```python
if __name__ == '__main__':
    main_yx(0.95)
    main_yx(0.85)
    main_yx(0.75)
    main_yx(0.65)
```

### 7 Discussions

#### 7.1 HU normalization

​	For HU normalization map, we have various choices. In this model, for HU not in [-1000,500], we take HU <-1000 to -1000 and HU >500 to 500 before normalization, then use a linear map to convert [-1000,500] to [0,255] and then make all values integers. We can actually choose another interval before normalization, say [-1000,400], [-1500, 1500] and so on. The length of the interval decides the accuracy of our transformation. Due to limited number jpg pixels can attain, longer interval reduces accuracy to differentiate nearby HU values, thus  the interval should be as short as possible. However, it is reasonable to make a distinction between different parts of a human body, thus we need to include -1000(air) and +400(cancellous bones). When testing on nodule HU values, we find that they are around 300, not far from 400, thus we extend the interval to 500 to avoid their normalized value too close to 255.

#### 7.2 CSV vs JPG

​	Since for each valid instance, we generate 4 files, we can make some comparisons based on them. For example, for covariates, we can consider either jpg files or csv files. The difference is that csv files contain the raw HU values without normalization, thus has wider range and no loss of HU information, while jpg files convert each HU value to an integer in [0,255], which means jpg files are at risk of losing information. 

#### 7.3 One channel vs Three channels

​	By thresholding, we have to lose some information. In order to maintain enough information from the original dicom file, we use a map to take HU values to a certain interval, and then convert them to JPG images. During this process, we try two ways: one is to map to one-channel images, the other is to map to three channel ones.  In the three-channel version, we use different colors to represent different range of HU values. Specifically, we first map all HU values lower than -1024 to -1024, and higher than 511 to 511. We choose the range to be 3 * 512 = 3 * 256 * 2, so that each HU value will only be squeezed half, not losing so much information. Then, we take the maps from revised HU values to three channels respectively, i.e.: [-1024, -513] -> [0,255] (red), [-512, -1] -> [0.255] (green) and [0,511] -> [0,255] (blue). However, when we compare the training results, three-channel ones are not as good as one-channel ones, although we only have [-1000,500] ->[0,255] for one-channel images, in which we have to squeeze the HU values more. The reason for this may be that while three-channel images represent HU values well, the "low-middle-high" logic may be difficult to understand for the model.

#### 7.4 Small Nodules Inclusion and Evaluation

​	Most articles using the LIDC-IDRI dataset do not include small nodules. One reason is that xml files providing ground truth information merely contain centroid coordinates of small nodules, whose diameter is less than 3mm. However, when we tested our first models disregarding small nodules, we found that some of the results given by our models provide more nodules than the ground truth label image with nodule masks excluding small ones. In order to know whether these excessive nodules are small nodules detected by our trained model, or simply some mistaken results, we would like to also include information about small nodules in our evaluation, or even, training process. Therefore, we include centroids and draw some imaginary small circles to represent them, since we have no knowledge about their shapes or exact diameters. Furthermore, to distinct them from labelled nodules whose diameters are at least 3mm, we choose gray color(value 127) to draw small nodules.